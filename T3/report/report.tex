\documentclass[conference]{IEEEtran}
\IEEEoverridecommandlockouts

\usepackage{cite}
\usepackage[portuguese,brazil]{babel}
\usepackage{amsmath, amssymb, amsfonts}
\usepackage[utf8]{inputenc}
\usepackage{algorithmic}
\usepackage{graphicx}
\usepackage{textcomp}
\usepackage{subfig}
\usepackage{diagbox}
\usepackage{ctable}
\def\BibTeX{{\rm B\kern-.05em{\sc i\kern-.025em b}\kern-.08em
    T\kern-.1667em\lower.7ex\hbox{E}\kern-.125emX}}
\begin{document}

\title{MO444 - Machine Learning - RelatÃ³rio da Atividade \#3 Grupo 12}

\author{\IEEEauthorblockN{BÃ¡rbara Caroline Benato}
\IEEEauthorblockA{
RA 192865\\
barbarabenato@gmail.com}
\and
\IEEEauthorblockN{Breno Leite}
\IEEEauthorblockA{
RA 192863\\
brenolleite@gmail.com}}

\maketitle

\section{Introdução}

A terceira atividade da disciplina de \textit{Machine Learning} (MO444) tem como objetivo explorar técnicas não-supervisionadas, a fim de se encontrar caracterí­sticas semelhantes em uma base de dados, em específico caracterí­sticas semelhantes na estrutura organizacional da base de dados.

O objetivo deste relatório é apresentar os experimentos que foram desenvolvidos com intuito de, principalmente, entender a base de dados não-supervisionada em questão, bem como encontrar o agrupamento mais adequado, capaz de representar as estruturas organizacionais da base de dados utilizando métodos de agrupamento, métricas para avaliação, bem como redução do espaço de características.

O mesmo Ã© dividido em Seções. Na Seção \ref{sec:meto}, alguns dados interessantes sobre a base de dados são mostrados, bem como a metodologia empregada e as atividades desenvolvidas. Por fim, a Seção \ref{sec:exp} apresenta os experimentos e as conclusões do trabalho.

\section{Materiais e Métodos} \label{sec:meto}

Os materiais, como a base de dados e pacote utilizados, e a metodologia empregados no presente trabalho são descritos a seguir.

\subsection{Base de dados} \label{sec:base}

A base de dados é não-supervisionada, ou seja, não apresenta informação de classe para as amostras. Desta forma, é utilizada para a validação dos estudos e objetivos propostos para o presente trabalho. A base de dados é composta por documentos de texto que apresentam certo grau de similaridade entre eles. Cada documento é representado por um vetor de características, ou seja, um histograma de \emph{hits} de uma abordagem \emph{Bag-of-Words}. A base apresenta $19.924$ documentos, em que cada documento é disponibilizado como um vetor de $2.209$ dimensões.

\subsection{Pacote \textit{Scikit-Learn}} \label{sec:pac}

Os modelos desenvolvidos que utilizam o algoritmo \emph{K-Means} foram implementados utilizando funções do pacote \emph{Scikit-Learn}, em linguagem de programação \emph{Python}. A função \emph{KMeans} foi empregada, a fim de se obter os agrupamentos, bem como fornecer os centroides para extração das métricas.

A configuração dos parametros da função \emph{KMeans} foi dada da seguinte forma:
\begin{itemize}
	\footnotesize \item \textit{n\_clusters}: A quantidade de agrupamentos utilizadas foi definida a partir da analise de dados e metricas e serão discutidos mais adiante.
	\footnotesize \item \textit{init}: k-means++, que sugere uma inicialização seguindo distribuição uniforme, uma vez que a inicialização randomica pode ser mal inicializada e dois centroides cairem num mesmo agrupamento, por exemplo.
	\footnotesize \item \textit{n\_init}: $10$, quantidade de vezes que as sementes serão inicializadas. A melhor delas Âé escolhida, evitando uma mÂá InicializaÃ§Ã£o.
	\footnotesize \item \textit{max\_iter}: $300$, numero máximo de iterações.
	\footnotesize \item \textit{tol}: $0,0001$, valor de tolerancia para convergencia, ou seja, valor em que um centroides muda entre iterações.
	\footnotesize \item \textit{precompute\_distances}: Verdadeiro, computa as distancias anteriormente para reduzir o tempo de processamento.
	\footnotesize \item \textit{random\_state}: Nenhum tipo de gerador de randomização é adicionado, ou seja, é utilizada a função da biblioteca \emph{numpy}.
	\footnotesize \item \textit{algorithm}: como utilizamos vetor de características com código esparso, optamos por usar o algoritmo classico do \emph{K-Means}.
\end{itemize}

Para o preprocessamento, as funções \emph{scale}, \emph{normalize} e \emph{StandardScaler} do mesmo pacote, o Scikit-Learn, foram utilizadas com seus parametros originais.

Como método de redução, o algoritmo \emph{Principal Component Analysis}-(PCA) foi utilizado do pacote, alterando apenas o número de componentes principais conforme os experimentos.

Para avaliar os agrupamentos formados no espaço de alta dimensão, optou-se por utilizar a métricas que mede a silhueta de cada agrupamento com função \emph{silhouette\_score}, bem como a função \emph{pairwise\_distances\_argmin\_min}, responsavel por retornar o argumento mínimo da menor distancias entre dois pares de pontos. Uma função semelhante foi utilizada do pacote \emph{Scipy} para computar distancias.

Para visualizar a projeção das amostras no espaço de alta dimensão, utilizou-se o algoritmo \emph{t-SNE}, um algoritmo capaz de projetar num espaço de menor dimensão (normalmente, 2), as amostras segundo a proximidade delas no espaço de alta dimensão com a função \emph{TSNE}.


\subsection{Metodologia} \label{sec:met} 

Com o intuito de tentar encontrar as estruturas de organização dos documentos, primeiramente algumas abordagens foram testadas para descobrir o número de agrupamentos que tais dados apresentavam num espaço de alta dimensão. Para isso, o algoritmo \emph{K-Means} foi utilizado juntamente com algumas metricas para avaliação dos grupos.

Posteriormente, os medóides de alguns grupos foram analisados com o objetivo de encontrar alguma similaridade no dado, ou seja, na informação presente no documento, entre elementos próximos no agrupamento.

Um metodo para avaliar a variancia de cada agrupamento foi proposto, para validar a qualidade dos agrupamentos conforme seus dados.

Após a análise inicial, aplicou-se o algoritmo PCA. Assim, entende-se que o PCA foi responsável por manter a informação mais relevante e discriminante para os agrupamentos. Foram testados valores de PCA para algumas variancias nos dados iniciais.

Assim, os experimentos do presente trabalho estão divididos da seguinte forma: 

\begin{itemize}
	\item \small \textbf{Análise da quantidade de agrupamentos:} Neste experimento, é realizada a análise de algumas métricas com o intuito de encontrar a quantidade de agrupamento dos dados, ou seja, a estrutura em que os documentos estão organizados.
	
	\item \small \textbf{Análise dos documentos segundo os medóides:} Para esta etapa, o objetivo é verificar o conteúdo dos documentos mais próximos ao centro de cada agrupamento e verificar se existe alguma similaridade entre eles.  
	
	\item \small \textbf{Checagem da qualidade dos agrupamentos:} Um método para avaliar a qualidade dos agrupamentos é explorado conforme a variancia dos dados.
	
	\item \small \textbf{Redução de dimensionalidade:} Para esse experimentos, considerou-se a redução da dimensionalidade do problema, a fim de tentar encontrar os agrupamentos e, assim a quantidade de classes.
	
	\item \small \textbf{Visualização dos dados:} Por fim, após a dimensionalidade ter sido reduzida, os dados foram projetados em um espaço de menos dimensão para a visualização dos agrupamentos.
	
\end{itemize}

\section{Experimentos e Discuss~oes} \label{sec:exp}

Esta seção tem como objetivo mostrar os resultados obtidos através da análises da base de dados, bem como os experimentos realizados com o intuito de encontrar a quantidade de classes do problema em questão. 

Como etapa de pré-processamento dos vetores de características da base de dados, algumas técnicas foram consideradas e a biblioteca \emph{Scikit-Learn} foi utilizada. Primeiramente, os dados foram normalizados utilizando a constante de regularizaç~ao $l2$. Ap´os a normalizaç~ao, os dados foram escalados utilizando a média dos dados. 

\subsection{Análise da quantidade de agrupamentos}

Para tentar encontrar a quantidade de agrupamento dos dados, primeiramente escolheu-se utilizar a metrica que calcula a silhueta dos agrupamentos, isso ´e, a forma dos agrupamentos e como eles t~ao se concentrando. Para um valor correto de n´umero de agrupamentos, o valor da medida silhueta deve ser pr´oximo de $1$, bem como a dissimilaridade proximo de $0$. Assim, testou-se o valor da medida de silhueta para $0, 5, 10, 15, ..., 100$. Os valores s~ao apresentados no gr´afico abaixo.

\begin{figure}[!h]
	\centering
	{
		\setlength{\fboxsep}{1pt}
		\setlength{\fboxrule}{1pt}
		\fbox{\includegraphics[scale=0.45]{Images/silhueta.png}}
	}
	\caption{\small Gr~afico da metrica silhueta pelo n´umero de agrupamentos.}
	\label{fig:confusion}
\end{figure}



\subsection{Análise dos documentos segundo os medóides}

\subsection{Checagem da qualidade dos agrupamentos}

\subsection{Redução de dimensionalidade}

\subsection{Visualização dos dados}

\section{Conclus~ao}

\end{document}

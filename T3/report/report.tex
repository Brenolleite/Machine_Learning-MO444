\documentclass[conference]{IEEEtran}
\IEEEoverridecommandlockouts

\usepackage{cite}
\usepackage[portuguese,brazil]{babel}
\usepackage{amsmath, amssymb, amsfonts}
\usepackage[utf8]{inputenc}
\usepackage{algorithmic}
\usepackage{graphicx}
\usepackage{textcomp}
\usepackage{subfig}
\usepackage{diagbox}
\usepackage{ctable}
\def\BibTeX{{\rm B\kern-.05em{\sc i\kern-.025em b}\kern-.08em
    T\kern-.1667em\lower.7ex\hbox{E}\kern-.125emX}}
\begin{document}

\title{MO444 - Machine Learning - Relatório da Atividade \#3 Grupo 12}

\author{\IEEEauthorblockN{Bárbara Caroline Benato}
\IEEEauthorblockA{
RA 192865\\
barbarabenato@gmail.com}
\and
\IEEEauthorblockN{Breno Leite}
\IEEEauthorblockA{
RA 192863\\
brenolleite@gmail.com}}

\maketitle

\section{Introdução}

A terceira atividade da disciplina de \textit{Machine Learning} (MO444) tem como objetivo explorar técnicas não-supervisionadas, a fim de se encontrar características semelhantes em uma base de dados, em específico características semelhantes na estrutura organizacional da base de dados.

O objetivo deste relatório é apresentar os experimentos que foram desenvolvidos com intuito de, principalmente, entender a base de dados em questão, bem como encontrar os agrupamentos mais adequados, capaz de representar as estruturas organizacionais da base de dados utilizando métodos de agrupamento, métricas para avaliação, bem como redução do espaço de características.

O mesmo é dividido em Seções. Na Seção \ref{sec:meto}, alguns dados interessantes sobre a base de dados são mostrados, bem como a metodologia empregada e as atividades desenvolvidas. Por fim, a Seção \ref{sec:exp} apresenta os experimentos e as conclusões do trabalho.

\section{Materiais e Métodos} \label{sec:meto}

Os materiais, como a base de dados e pacote utilizados, e a metodologia empregados no presente trabalho são descritos a seguir.

\subsection{Base de dados} \label{sec:base}

A base de dados é não-supervisionada, ou seja, não apresenta informação de classe para as amostras. Desta forma, é utilizada para a validação dos estudos e objetivos propostos para o presente trabalho. A base de dados é composta por documentos de texto que apresentam certo grau de similaridade entre eles. Cada documento é representado por um vetor de características, ou seja, um histograma de \emph{hits} de uma abordagem \emph{Bag-of-Words}. A base apresenta $19.924$ documentos, em que cada documento é disponibilizado como um vetor de $2.209$ dimensões.

\subsection{Pacote \textit{Scikit-Learn}} \label{sec:pac}

Os modelos desenvolvidos que utilizam o algoritmo \emph{K-Means} foram implementados utilizando funções do pacote \emph{Scikit-Learn}, em linguagem de programação \emph{Python}. A função \emph{KMeans} foi empregada, a fim de se obter os agrupamentos, bem como fornecer os centroides para extração das métricas.

A configuração dos parametros da função \emph{KMeans} foi dada da seguinte forma:
\begin{itemize}
	\footnotesize \item \textit{n\_clusters}: A quantidade de agrupamentos utilizadas foi definida a partir da analise de dados e métricas e serão discutidos mais adiante.
	\footnotesize \item \textit{init}: k-means++, que sugere uma inicialização seguindo distribuição uniforme, uma vez que a inicialização randômica pode ser mal inicializada e dois centróides caírem num mesmo agrupamento, por exemplo.
	\footnotesize \item \textit{n\_init}: $10$, quantidade de vezes que as sementes serão inicializadas. A melhor delas é escolhida, evitando uma má Inicialização.
	\footnotesize \item \textit{max\_iter}: $300$, numero máximo de iterações.
	\footnotesize \item \textit{tol}: $0,0001$, valor de tolerância para convergência, ou seja, valor em que um centroides muda entre iterações.
	\footnotesize \item \textit{precompute\_distances}: Verdadeiro, computa as distancias anteriormente para reduzir o tempo de processamento.
	\footnotesize \item \textit{random\_state}: Nenhum tipo de gerador de randomização é adicionado, ou seja, é utilizada a função da biblioteca \emph{numpy}.
	\footnotesize \item \textit{algorithm}: como utilizamos vetor de características com código esparso, optamos por usar o algoritmo classico do \emph{K-Means}.
\end{itemize}

Para o pré-processamento, as funções \emph{scale}, \emph{normalize} e \emph{StandardScaler} do mesmo pacote, o Scikit-Learn, foram utilizadas com seus parâmetros originais.

Como método de redução, o algoritmo \emph{Principal Component Analysis}-(PCA) foi utilizado do pacote, alterando apenas o número de componentes principais conforme os experimentos.

Para avaliar os agrupamentos formados no espaço de alta dimensão, optou-se por utilizar a métricas que mede a silhueta de cada agrupamento com função \emph{silhouette\_score}, bem como a função \emph{pairwise\_distances\_argmin\_min}, responsavel por retornar o argumento mínimo da menor distancias entre dois pares de pontos. Uma função semelhante foi utilizada do pacote \emph{Scipy} para computar distancias.

Para visualizar a projeção das amostras no espaço de alta dimensão, utilizou-se o algoritmo \emph{t-SNE}, um algoritmo capaz de projetar num espaço de menor dimensão (normalmente, 2), as amostras segundo a proximidade delas no espaço de alta dimensão com a função \emph{TSNE}.


\subsection{Metodologia} \label{sec:met} 

Com o intuito de tentar encontrar as estruturas de organização dos documentos, primeiramente algumas abordagens foram testadas para descobrir o número de agrupamentos que tais dados apresentavam num espaço de alta dimensão. Para isso, o algoritmo \emph{K-Means} foi utilizado juntamente com algumas métricas para avaliação dos grupos.

Posteriormente, os medóides de alguns grupos foram analisados com o objetivo de encontrar alguma similaridade no dado, ou seja, na informação presente no documento, entre elementos próximos no agrupamento.

Um método para avaliar a variância de cada agrupamento foi proposto, para validar a qualidade dos agrupamentos conforme seus dados.

Após a análise inicial, aplicou-se o algoritmo PCA. Assim, entende-se que o PCA foi responsável por manter a informação mais relevante e discriminante para os agrupamentos. Foram testados diferentes valores de componentes no PCA, de forma que uma certa variância nos dados fosse mantida.

Assim, os experimentos do presente trabalho estão divididos da seguinte forma: 

\begin{itemize}
	\item \small \textbf{Análise da quantidade de agrupamentos:} Neste experimento, é realizada a análise de algumas métricas com o intuito de encontrar a quantidade de agrupamento dos dados, ou seja, a estrutura em que os documentos estão organizados.
	
	\item \small \textbf{Análise dos documentos:} Para esta etapa, o objetivo é verificar o conteúdo dos documentos mais próximos ao centro de cada agrupamento e verificar se existe alguma similaridade entre eles. Além de estudar a composição dos documentos com intuito de melhor entender e agrupar a base de dados proposta.  
	
	\item \small \textbf{Checagem da qualidade dos agrupamentos:} Um método para avaliar a qualidade dos agrupamentos é proposta, de forma a complementar os experimentos mostrados anteriormente.
	
	\item \small \textbf{Redução de dimensionalidade:} Para esses experimentos, considerou-se a redução da dimensionalidade do problema, a fim de tentar encontrar os agrupamentos e, assim a quantidade de classes.
	
	\item \small \textbf{Visualização dos dados:} Por fim, após a dimensionalidade ter sido reduzida, os dados foram projetados em um espaço de menos dimensão para a visualização dos agrupamentos.
	
\end{itemize}

\section{Experimentos e Discussões} \label{sec:exp}

Esta seção tem como objetivo mostrar os resultados obtidos através da análises da base de dados, bem como os experimentos realizados com o intuito de encontrar a quantidade de classes do problema em questão. 

Como etapa de pré-processamento dos vetores de características da base de dados, algumas técnicas foram consideradas e a biblioteca \emph{Scikit-Learn} foi utilizada. Primeiramente, os dados foram normalizados utilizando a constante de regularização $l2$. Após a normalização, os dados foram escalados utilizando a média dos dados. 

\subsection{Análise da quantidade de agrupamentos}

Para tentar encontrar a quantidade de agrupamento dos dados, primeiramente escolheu-se utilizar o método \emph{Elbow}, que calcula a soma da distorção, ou seja, o quanto cada elemento se distancia dos outros elementos do seu agrupamento. Assim, a ideia do método é calcular a distorção dos elementos para diferentes quantidades de grupos e encontrar uma grande queda no gráfico, que representa um valor adequado e com pouca distorção para todos os grupos. Testou-se o valor dos grupos para $0, 5, 10, 15, ..., 100$. Os valores são apresentados no gráfico \ref{fig:elbow}.

\begin{figure}[!h]
	\centering
	{
		\fbox{\includegraphics[scale=0.45]{Images/elbow.png}}
	}
	\caption{\small Gráfico do método \emph{Elbow} para os quantidades de grupos diferentes.}
	\label{fig:elbow}
\end{figure}

A partir do gráfico, é possível visualizar que existe uma distorção muito pequena, de $2,5$ aproximadamente, para o número mínimo e máximo de grupos testados, $0$ e $100$, respectivamente. Embora não exista uma grande queda no gráfico, é possível observar uma queda mais acentuada entre $20$ e $40$, e duas quedas menores: uma entre $60$ até $80$ e $80$ até $100$. Desta forma, entendemos que tais intervalos são promissores para escolha de número de agrupamentos.

Como existiam 3 possíveis valores para números de grupos, escolheu-se utilizar outra métrica para tentar encontrar o melhor número de grupos. A métrica escolhida calcula a silhueta dos agrupamentos, isso é, a forma dos agrupamentos e como eles se concentram. Para um valor correto de número de agrupamentos, o valor da medida silhueta deve ser próximo de $1$. Assim, testou-se o valor da medida de silhueta para $0, 5, 10, 15, ..., 100$. Os valores são apresentados no gráfico \ref{fig:silhueta}.

\begin{figure}[!h]
	\centering
	{
		\fbox{\includegraphics[scale=0.45]{Images/silhueta.png}}
	}
	\caption{\small Gráfico da métrica silhueta pelo número de agrupamentos.}
	\label{fig:silhueta}
\end{figure}

No gráfico, é possível observar que os valores de silhueta não ficaram próximos a $1$. Um possível motivo pode ser atribuído ao ``mal da dimensionalidade'', ou seja, devido a alta dimensionalidade do problema (o vetor de características apresenta dimensão de $2209$) os dados estão muito dispersos e não são capazes de apresentar uma silhueta bem definida. Contudo, ainda é possível observar que os valores mais promissores referem-se ao número de agrupamentos de $31$, $79$ e $92$ grupos. Tais valores encontram-se dentro dos intervalos sugeridos pelo método \textit{Elbow}. Para realizar os experimentos, optou-se por utilizar os melhores valores de silhueta: $31$ e $79$, o valor $92$ foi ignorado pois o mesmo é muito similar com o valor de $79$.


\subsection{Análise dos documentos}

Com o objetivo de tentar entender a estrutura dos dados, os documentos foram abertos e analisados. No cabeçalho de cada documento, foi possível notar a presença de um identificador denominado \emph{Newsgroup}. Tal identificador mostra algumas palavras-chave para identificar o conteúdo do documento, este identificador é muito utilizado em compartilhamento de arquivos de texto e servem para definir um tema para cada arquivo, muito similar com um tópico de um fórum \footnote{https://revistausenet.com/o-que-e-um-newsgroup-2/}. Um identificador \emph{newsgroup} normalmente é separados por classes e subclasses, sendo elas separadas por ponto dentro do nome, os mesmos podem ter até mais de uma subclasse por identificador. Desta forma, algumas análises puderam ser feitas neste caminho. O Gráfico \ref{fig:occurrence} apresenta as maiores ocorrências de \emph{Newsgroup}, considerando o conjunto palavras-chave que o compõe.

\begin{figure}[!h]
	\centering
	{
		\fbox{\includegraphics[scale=0.3]{Images/occurrence.png}}
	}
	\caption{\small Maiores ocorrências dos identificadores.}
	\label{fig:occurrence}
\end{figure}

O Gráfico \ref{fig:occurrence} mostra as $20$ primeiras maiores ocorrências dos \emph{Newsgroups}. As $20$ primeiras ocorrências apresentam valor maior do que $950$ e somam $62,42\%$ de todas as ocorrências. A partir da ocorrência $21$ o numero da ocorrência de cada \emph{Newsgroup} é menor do que $300$ ocorrências cada uma. Assim, tais dados sugerem que existam $20$ agrupamentos de maior densidade de informação, ou seja, que apresentem o mesmo \emph{Newsgroup}. Com relação ao agrupamento dos dados, imagina-se que as maiores ocorrências sugiram agrupamentos maiores de informação.

Outra análise foi feita para avaliar a ocorrência dos \emph{Newsgroups}. O Gráfico \ref{fig:occurrence_word} apresenta as maiores ocorrências de cada \emph{Newsgroup} considerando apenas a classe principal, ignorando todas as subclasses.

\begin{figure}[!h]
	\centering
	{
		\fbox{\includegraphics[scale=0.3]{Images/occurence_word.png}}
	}
	\caption{\small Maiores ocorrências de cada identificador.}
	\label{fig:occurrence_word}
\end{figure}

A partir do Gráfico \ref{fig:occurrence_word}, é possível visualizar que a grande parte dos documentos na base estão dentro de apenas 7 de um total de 141 classes principais. Desta forma, $95.98\%$ dos documentos estão distribuidos dentro dessas 7 classes.

\subsubsection{Análise dos medóides}


\begin{table}[h!]
	\centering	
	\begin{tabular}{cc} \toprule
		\textbf{Grupo} & \textbf{Documents} \\ \toprule 	
		relu                   & softmax               \\
		relu                   & sigmoid                \\
		tanh                   & softmax               \\
		tanh                   & sigmoid                \\
		sigmoid                & softmax             \\
		linear                 & softmax                \\
		linear                 & sigmoid                 \\
		softmax                & sigmoid              \\ \bottomrule      
	\end{tabular}
	\caption{\small Resultados de acurácia média dos $5$-\emph{folds} utilizando redes neurais com duas camadas escondidas.}
	\label{tab:2hl}
\end{table}



\subsection{Checagem da qualidade dos agrupamentos}


Para realizar uma checagem mais ampla dentro de cada agrupamento uma métrica diferente foi proposta, para cada agrupamento foi criado um histograma utilizando o número de ocorrência para cada \emph{Newsgroup} dentro do grupo criado. Desta forma, é esperado que a variância dos histogramas sejam altas pois um agrupamento deve conter muitos arquivos da mesma categoria. A Figura \ref{fig:clusters} mostra o resultado para três diferentes clusters (representados pelas cores) mostrando os quatro maiores valores dentro do histograma para $K = 31$ e $K = 79$. Clusters com maior variância foram escolhidos para este experimento.

\begin{figure}[!h]
	\centering
	{
		\fbox{\includegraphics[scale=0.25]{Images/clusters.png}}
	}
	\caption{\small Histograma com as quatro maiores categorias para três diferentes agrupamentos.}
	\label{fig:clusters}
\end{figure}

Pode-se perceber que os três agrupamentos quando $K = 31$ identificam muito bem a categoria principal, como por exemplo \emph{rec, talk, soc}, outro ponto interessante é que subclasses similares são agrupadas mesmo quando a classe principal é diferente, como por exemplo as classes \emph{sci.eletronics} - \emph{rec.autos.tech} ambos arquivos possuem o tema de tecnologia e acabaram no mesmo agrupamento, o mesmo acontece com as classes \emph{alt.atheism} - \emph{talk.religion.misc}.

Os resultados utilizando $K = 79$ foram bem similares, os agrupamentos continuaram focando nas classes principais. Desta forma, relacões entre diferentes categorias também aconteceram, como por exemplo \emph{sci.crypt} - \emph{comp.security}.


\subsection{Redução de dimensionalidade}

Com intuito de melhorar os agrupamentos foram feitos experimentos utilizando redução de dimensionalidade, a Figura \ref{fig:var_comp} mostra a distribuição da variância dos dados por número de componentes.

\begin{figure}[!h]
	\centering
	{
		\fbox{\includegraphics[scale=0.3]{Images/variancia.png}}
	}
	\caption{\small Variância por numero de componentes.}
	\label{fig:var_comp}
\end{figure}

Pode-se notar que acima de 1500 componentes $87\%$  da variância dos dados é mantida, para os experimentos deste trabalho foram utilizados os valores de 1424, 1608 e 1830 componentes, que são responsáveis por $85\%$, $90\%$ e $95\%$ da variância dos dados. A Figura \ref{fig:clusters_pca} mostra os histogramas obtidos pelos agrupamentos de maior variância utilizando o processo de redução de dimensionalidade antes do agrupamento. Em todos esses experimentos $K = 79$ foi utilizado, pois o mesmo mostrou melhores resultados.


\begin{figure}[!h]
	\centering
	{
		{
			\setlength{\fboxsep}{1pt}
			\setlength{\fboxrule}{1pt}
			\fbox{\includegraphics[scale=0.2]{Images/pca_95}}
		}
		\label{fig:pca95}
	}\smallskip
	\enskip
	{
		{
			\setlength{\fboxsep}{1pt}
			\setlength{\fboxrule}{1pt}
			\fbox{\includegraphics[scale=0.2]{Images/pca_90}}
		}
		\label{fig:pca90}
	}\smallskip
	\enskip
	{
		{
			\setlength{\fboxsep}{1pt}
			\setlength{\fboxrule}{1pt}
			\fbox{\includegraphics[scale=0.25]{Images/pca_85}}
		}
		\label{fig:pca85}
	}
	\caption{Histogramas utilizando diferentes reduções de dimensionalidade.}
	\label{fig:clusters_pca}
\end{figure}



\subsection{Visualização dos dados}

\section{Conclusão}

\end{document}
